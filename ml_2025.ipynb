{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon ML Challenge 2025 - Price Prediction (Optimized Solution)\n",
    "**Goal:** Predict product prices from catalog content with SMAPE < 40%\n",
    "\n",
    "**Strategy:**\n",
    "- Advanced feature engineering (target encoding, text features, categories)\n",
    "- Ensemble modeling (LightGBM + XGBoost + CatBoost)\n",
    "- Weighted averaging based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nTarget (price) range: ${train_df['price'].min():.2f} - ${train_df['price'].max():.2f}\")\n",
    "print(f\"Mean price: ${train_df['price'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Pipeline\n",
    "Extract comprehensive features from catalog content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(text):\n",
    "    \"\"\"Extract all relevant features from catalog content\"\"\"\n",
    "    features = {}\n",
    "    text_str = str(text).lower()\n",
    "    \n",
    "    # 1. Quantity features\n",
    "    value_match = re.search(r'value:\\s*([\\d.]+)', text_str)\n",
    "    features['value'] = float(value_match.group(1)) if value_match else 0.0\n",
    "    \n",
    "    unit_match = re.search(r'unit:\\s*(\\w+(?:\\s+\\w+)?)', text_str)\n",
    "    features['unit'] = unit_match.group(1) if unit_match else 'unknown'\n",
    "    \n",
    "    pack_match = re.search(r'pack of (\\d+)|(\\d+)\\s*pack', text_str, re.IGNORECASE)\n",
    "    features['pack_size'] = int(pack_match.group(1) or pack_match.group(2)) if pack_match else 1\n",
    "    features['total_quantity'] = features['value'] * features['pack_size']\n",
    "    \n",
    "    # 2. Brand extraction\n",
    "    name_match = re.search(r'item name:\\s*([^,\\n]+?)(?:,|\\n)', text_str)\n",
    "    if name_match:\n",
    "        brand = name_match.group(1).strip().split()[0]\n",
    "        features['brand'] = brand if brand else 'unknown'\n",
    "    else:\n",
    "        features['brand'] = 'unknown'\n",
    "    \n",
    "    # 3. Text statistics\n",
    "    words = text_str.split()\n",
    "    features['char_count'] = len(text_str)\n",
    "    features['word_count'] = len(words)\n",
    "    features['bullet_count'] = len(re.findall(r'bullet point \\d+:', text_str))\n",
    "    features['avg_word_len'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    # 4. Premium keywords (count occurrences)\n",
    "    premium_kw = ['premium', 'luxury', 'deluxe', 'gourmet', 'organic', 'natural',\n",
    "                  'professional', 'artisan', 'imported', 'certified', 'exclusive']\n",
    "    features['premium_count'] = sum(text_str.count(kw) for kw in premium_kw)\n",
    "    \n",
    "    # 5. Category detection\n",
    "    features['is_food'] = int(any(kw in text_str for kw in ['food', 'snack', 'beverage', 'coffee', 'tea', 'spice']))\n",
    "    features['is_electronics'] = int(any(kw in text_str for kw in ['electronic', 'battery', 'charger', 'cable']))\n",
    "    features['is_health'] = int(any(kw in text_str for kw in ['health', 'beauty', 'vitamin', 'supplement', 'care']))\n",
    "    features['is_home'] = int(any(kw in text_str for kw in ['home', 'kitchen', 'household', 'cleaning']))\n",
    "    \n",
    "    # 6. Size indicators\n",
    "    features['has_ounce'] = int('ounce' in text_str or ' oz' in text_str)\n",
    "    features['has_pound'] = int('pound' in text_str or ' lb' in text_str)\n",
    "    features['has_gram'] = int('gram' in text_str or ' g ' in text_str)\n",
    "    \n",
    "    # 7. Quality indicators\n",
    "    features['digit_count'] = sum(c.isdigit() for c in text_str)\n",
    "    features['upper_ratio'] = sum(c.isupper() for c in str(text)) / len(str(text)) if len(str(text)) > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features from training data...\")\n",
    "train_features = train_df['catalog_content'].progress_apply(extract_all_features).apply(pd.Series)\n",
    "train_df = pd.concat([train_df, train_features], axis=1)\n",
    "\n",
    "print(\"Extracting features from test data...\")\n",
    "test_features = test_df['catalog_content'].progress_apply(extract_all_features).apply(pd.Series)\n",
    "test_df = pd.concat([test_df, test_features], axis=1)\n",
    "\n",
    "print(f\"âœ“ Extracted {len(train_features.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding for brand (CRITICAL for performance)\n",
    "print(\"Creating brand target encoding...\")\n",
    "brand_stats = train_df.groupby('brand')['price'].agg([\n",
    "    ('brand_mean', 'mean'),\n",
    "    ('brand_median', 'median'),\n",
    "    ('brand_std', 'std'),\n",
    "    ('brand_count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "train_df = train_df.merge(brand_stats, on='brand', how='left')\n",
    "test_df = test_df.merge(brand_stats, on='brand', how='left')\n",
    "\n",
    "# Fill missing for unseen brands\n",
    "for col in ['brand_mean', 'brand_median', 'brand_std']:\n",
    "    test_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "test_df['brand_count'].fillna(1, inplace=True)\n",
    "\n",
    "print(\"âœ“ Brand encoding complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding for unit\n",
    "print(\"Creating unit target encoding...\")\n",
    "unit_stats = train_df.groupby('unit')['price'].agg([\n",
    "    ('unit_mean', 'mean'),\n",
    "    ('unit_median', 'median'),\n",
    "    ('unit_count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "train_df = train_df.merge(unit_stats, on='unit', how='left')\n",
    "test_df = test_df.merge(unit_stats, on='unit', how='left')\n",
    "\n",
    "for col in ['unit_mean', 'unit_median']:\n",
    "    test_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "test_df['unit_count'].fillna(1, inplace=True)\n",
    "\n",
    "print(\"âœ“ Unit encoding complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_train = tfidf.fit_transform(train_df['catalog_content'].fillna(''))\n",
    "tfidf_test = tfidf.transform(test_df['catalog_content'].fillna(''))\n",
    "\n",
    "print(f\"âœ“ TF-IDF: {tfidf_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "feature_cols = [\n",
    "    'value', 'pack_size', 'total_quantity',\n",
    "    'brand_mean', 'brand_median', 'brand_std', 'brand_count',\n",
    "    'unit_mean', 'unit_median', 'unit_count',\n",
    "    'char_count', 'word_count', 'bullet_count', 'avg_word_len',\n",
    "    'premium_count', 'is_food', 'is_electronics', 'is_health', 'is_home',\n",
    "    'has_ounce', 'has_pound', 'has_gram', 'digit_count', 'upper_ratio'\n",
    "]\n",
    "\n",
    "X_train_num = train_df[feature_cols].values\n",
    "X_train = np.hstack([X_train_num, tfidf_train.toarray()])\n",
    "\n",
    "X_test_num = test_df[feature_cols].values\n",
    "X_test = np.hstack([X_test_num, tfidf_test.toarray()])\n",
    "\n",
    "y_train = np.log1p(train_df['price'].values)  # Log transform\n",
    "\n",
    "print(f\"Feature matrix: {X_train.shape} (train), {X_test.shape} (test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=RANDOM_SEED\n",
    ")\n",
    "print(f\"Train: {X_tr.shape[0]:,}, Validation: {X_val.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Modeling\n",
    "Train 3 gradient boosting models and combine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(np.abs(y_true - y_pred) / denominator) * 100\n",
    "\n",
    "print(\"âœ“ SMAPE function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "params_lgb = {\n",
    "    'objective': 'regression', 'metric': 'mae', 'boosting_type': 'gbdt',\n",
    "    'num_leaves': 63, 'learning_rate': 0.03, 'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85, 'bagging_freq': 5, 'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1, 'reg_lambda': 0.1, 'verbose': -1,\n",
    "    'random_state': RANDOM_SEED, 'n_jobs': -1\n",
    "}\n",
    "\n",
    "train_lgb = lgb.Dataset(X_tr, label=y_tr)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb)\n",
    "\n",
    "model_lgb = lgb.train(\n",
    "    params_lgb, train_lgb, num_boost_round=2000,\n",
    "    valid_sets=[train_lgb, val_lgb], valid_names=['train', 'valid'],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n",
    ")\n",
    "\n",
    "y_val_pred_lgb = np.expm1(model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration))\n",
    "y_val_actual = np.expm1(y_val)\n",
    "smape_lgb = calculate_smape(y_val_actual, y_val_pred_lgb)\n",
    "print(f\"âœ“ LightGBM - SMAPE: {smape_lgb:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "params_xgb = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'mae',\n",
    "    'max_depth': 7, 'learning_rate': 0.03, 'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85, 'min_child_weight': 3,\n",
    "    'reg_alpha': 0.1, 'reg_lambda': 0.1, 'tree_method': 'hist',\n",
    "    'random_state': RANDOM_SEED, 'n_jobs': -1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "model_xgb = xgb.train(\n",
    "    params_xgb, dtrain, num_boost_round=2000,\n",
    "    evals=[(dtrain, 'train'), (dval, 'valid')],\n",
    "    early_stopping_rounds=100, verbose_eval=200\n",
    ")\n",
    "\n",
    "y_val_pred_xgb = np.expm1(model_xgb.predict(dval, iteration_range=(0, model_xgb.best_iteration)))\n",
    "smape_xgb = calculate_smape(y_val_actual, y_val_pred_xgb)\n",
    "print(f\"âœ“ XGBoost - SMAPE: {smape_xgb:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "model_catb = cb.CatBoostRegressor(\n",
    "    iterations=2000, learning_rate=0.03, depth=7, l2_leaf_reg=3,\n",
    "    random_seed=RANDOM_SEED, verbose=200, early_stopping_rounds=100,\n",
    "    task_type='CPU', thread_count=-1\n",
    ")\n",
    "\n",
    "model_catb.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=200)\n",
    "\n",
    "y_val_pred_catb = np.expm1(model_catb.predict(X_val))\n",
    "smape_catb = calculate_smape(y_val_actual, y_val_pred_catb)\n",
    "print(f\"âœ“ CatBoost - SMAPE: {smape_catb:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble with optimal weights\n",
    "weights = np.array([1/smape_lgb, 1/smape_xgb, 1/smape_catb])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "print(f\"\\nEnsemble weights: LGB={weights[0]:.3f}, XGB={weights[1]:.3f}, CAT={weights[2]:.3f}\")\n",
    "\n",
    "y_val_pred_ens = (\n",
    "    weights[0] * y_val_pred_lgb +\n",
    "    weights[1] * y_val_pred_xgb +\n",
    "    weights[2] * y_val_pred_catb\n",
    ")\n",
    "\n",
    "smape_ens = calculate_smape(y_val_actual, y_val_pred_ens)\n",
    "print(f\"\\nðŸŽ¯ Ensemble SMAPE: {smape_ens:.2f}%\")\n",
    "print(f\"Improvement: {min(smape_lgb, smape_xgb, smape_catb) - smape_ens:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full data\n",
    "print(\"Training final models on full training data...\")\n",
    "\n",
    "# LightGBM\n",
    "full_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "final_lgb = lgb.train(params_lgb, full_lgb, num_boost_round=model_lgb.best_iteration)\n",
    "\n",
    "# XGBoost\n",
    "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
    "final_xgb = xgb.train(params_xgb, dtrain_full, num_boost_round=model_xgb.best_iteration)\n",
    "\n",
    "# CatBoost\n",
    "final_catb = cb.CatBoostRegressor(\n",
    "    iterations=model_catb.best_iteration_, learning_rate=0.03,\n",
    "    depth=7, l2_leaf_reg=3, random_seed=RANDOM_SEED, verbose=0\n",
    ")\n",
    "final_catb.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ“ All models trained on full data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ensemble predictions\n",
    "print(\"Generating ensemble predictions...\")\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "y_test_lgb = np.expm1(final_lgb.predict(X_test, num_iteration=final_lgb.best_iteration))\n",
    "y_test_xgb = np.expm1(final_xgb.predict(dtest, iteration_range=(0, final_xgb.best_iteration)))\n",
    "y_test_catb = np.expm1(final_catb.predict(X_test))\n",
    "\n",
    "y_test_final = (\n",
    "    weights[0] * y_test_lgb +\n",
    "    weights[1] * y_test_xgb +\n",
    "    weights[2] * y_test_catb\n",
    ")\n",
    "\n",
    "y_test_final = np.maximum(y_test_final, 0.01)  # Ensure positive\n",
    "\n",
    "print(f\"Predictions: {len(y_test_final):,}\")\n",
    "print(f\"Range: ${y_test_final.min():.2f} - ${y_test_final.max():.2f}\")\n",
    "print(f\"Mean: ${y_test_final.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': y_test_final\n",
    "})\n",
    "\n",
    "# Validate\n",
    "assert len(submission) == len(test_df)\n",
    "assert list(submission.columns) == ['sample_id', 'price']\n",
    "assert (submission['price'] > 0).all()\n",
    "assert submission['price'].isna().sum() == 0\n",
    "\n",
    "# Save\n",
    "submission.to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… SUBMISSION READY: test_out.csv\")\n",
    "print(f\"Expected SMAPE: ~{smape_ens:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Improvements:**\n",
    "1. âœ… Target encoding for brand & unit (captures price patterns)\n",
    "2. âœ… Comprehensive text features (1000 TF-IDF + metadata)\n",
    "3. âœ… Category detection (food, electronics, health, home)\n",
    "4. âœ… Ensemble of 3 models with optimal weighting\n",
    "5. âœ… Log transformation for better price distribution\n",
    "\n",
    "**Expected Performance:** SMAPE < 40%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPrWTwwK+88vSyJ+4EMKmtQ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
